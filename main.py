# labnote-ai-backend/main.py

import os
import logging
import datetime
import uuid
import re
import asyncio
import json
import redis.asyncio as redis
import sqlite3, datetime
import ollama
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import Optional, List, Dict
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from pathlib import Path
from rapidfuzz import fuzz
import git
from fastapi.middleware.cors import CORSMiddleware 

# Local imports
import rag_pipeline as rag_module
from agents import run_agent_team
from llm_utils import call_llm_api

# embedding
from rag_pipeline import get_embeddings

# .env íŒŒì¼ ë¡œë“œ ë° ë¡œê¹… ì„¤ì •
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼) ---
WORKFLOW_GUIDE_DATA = """
# Workflows Guide
## Design (ì„¤ê³„)
- WD010: General Design of Experiment (ì‹¤í—˜ì„¤ê³„ë²•(DOE)ì„ í™œìš©í•œ ë²”ìš©ì  ì‹¤í—˜ ì¡°ê±´ ìµœì í™”)
- WD020: Adaptive Laboratory Evolution Design (ë¬´ì‘ìœ„ ëŒì—°ë³€ì´ ë° ì¸ê³µ ì§„í™”ë¥¼ í†µí•œ í•˜í–¥ì‹ ì„¤ê³„)
- WD030: Growth Media Design (ë°ì´í„° ê¸°ë°˜ ì‹¤í—˜ ì„¤ê³„ë¥¼ í†µí•œ ê· ì£¼ ë°°ì–‘ìš© ì„±ì¥ ë°°ì§€ ìµœì í™”)
- WD040: Parallel Cell Culture/Fermentation Design (ë‹¨ë°±ì§ˆ, íš¨ì†Œ ëŒ€ëŸ‰ ë°°ì–‘ ë˜ëŠ” ê· ì£¼ í™œì„± í…ŒìŠ¤íŠ¸ ì¡°ê±´ ì„¤ê³„)
- WD050: DNA Oligomer Pool Design (ëª©í‘œ DNA ì„œì—´ ì¡°ë¦½ì„ ìœ„í•œ ì˜¬ë¦¬ê³ ë¨¸ í’€ ì„¤ê³„)
- WD060: Genetic Circuit Design (ë°”ì´ì˜¤ì„¼ì„œ, ë…¼ë¦¬ ê²Œì´íŠ¸ ë“± íŠ¹ì • ëª©ì ì˜ ìœ ì „ íšŒë¡œ ì„¤ê³„)
- WD070: Vector Design (í”Œë¼ìŠ¤ë¯¸ë“œ, BAC, YAC ë“± ë²¡í„° í˜•íƒœì˜ DNA êµ¬ì¶• ì„¤ê³„)
- WD080: Artificial Genome Design (ìœ ì „ì²´ ì••ì¶•, ì½”ëˆ ì¬ì„¤ê³„ ë“± ìƒˆë¡œìš´ ìœ ì „ì²´ ë””ìì¸)
- WD090: Genome Editing Design (CRISPR ê¸°ë°˜ ìœ ì „ì²´ í¸ì§‘ì„ ìœ„í•œ gRNA ì„¤ê³„)
- WD100: Protein Library Design (ë‹¨ë°±ì§ˆ í™œì„±, íŠ¹ì´ì„±, ë°œí˜„ ìµœì í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ê³„)
- WD110: De novo Protein/Enzyme Design (ë”¥ëŸ¬ë‹ ë„êµ¬ë¥¼ ì´ìš©í•œ ìƒˆë¡œìš´ ë‹¨ë°±ì§ˆ ë˜ëŠ” íš¨ì†Œ ì„¤ê³„)
- WD120: Retrosynthetic Pathway Design (ì—­í•©ì„± ë¶„ì„ì„ í†µí•œ ëª©í‘œ ëŒ€ì‚¬ì‚°ë¬¼ ìƒì‚° ê²½ë¡œ ì„¤ê³„)
- WD130: Pathway Library Design (ëŒ€ì‚¬ ê²½ë¡œ ê¸°ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ DNA ë¶€í’ˆ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ê³„)
## Build (êµ¬ì¶•)
- WB005: Nucleotide Quantification (UV í¡ê´‘ë„ ë° í˜•ê´‘ ë¶„ì„ì„ í†µí•œ í•µì‚° ì •ëŸ‰ ë° ìˆœë„ í‰ê°€)
- WB010: DNA Oligomer Assembly (DNA ì˜¬ë¦¬ê³ ë¨¸ í’€ë¡œë¶€í„° ì •í™•í•œ DNA ì„œì—´ ì¡°ë¦½)
- WB020: DNA Library Construction (DNA ëŒì—°ë³€ì´, ë©”íƒ€ê²Œë†ˆ, ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œì‘)
- WB025: Sequencing Library Preparation (ì°¨ì„¸ëŒ€ ì‹œí€€ì‹±(NGS)ì„ ìœ„í•œ DNA/RNA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„)
- WB030: DNA Assembly (ì—¬ëŸ¬ DNA ë‹¨í¸ì„ íŠ¹ì • ìˆœì„œë¡œ ì¡°ë¦½í•˜ì—¬ ìœ ì „ êµ¬ì¡°ë¬¼ ì œì‘)
- WB040: DNA Purification (ì»¬ëŸ¼, ë¹„ë“œ ë“±ì„ ì´ìš©í•´ ì¡° DNA ì¶”ì¶œë¬¼ì—ì„œ ê³ ìˆœë„ DNA ì •ì œ)
- WB045: DNA Extraction (ì„¸í¬ ìš©í•´ë¥¼ í†µí•´ ìƒë¬¼í•™ì  ìƒ˜í”Œë¡œë¶€í„° DNA ì¶”ì¶œ)
- WB050: RNA Extraction (ìœ ì „ì ë°œí˜„ ë¶„ì„ ë“±ì„ ìœ„í•´ ìƒë¬¼í•™ì  ìƒ˜í”Œì—ì„œ RNA ë¶„ë¦¬)
- WB060: DNA Multiplexing (ì‹ë³„ì„ ìœ„í•´ ì„¸í¬ì— ë°”ì½”ë“œë¥¼ í• ë‹¹í•˜ê³  NGSìš© DNA í’€ë§)
- WB070: Cell-free Mixture Preparation (ë¬´ì„¸í¬ ë°˜ì‘ì„ ìœ„í•œ ë§ˆìŠ¤í„° ìš©ì•¡ ë° ì„¸í¬ ì¶”ì¶œë¬¼ ì¤€ë¹„)
- WB080: Cell-free Protein/Enzyme Expression (ë¬´ì„¸í¬ ë°˜ì‘ ì‹œìŠ¤í…œì—ì„œ ëª©í‘œ ë‹¨ë°±ì§ˆ ë˜ëŠ” íš¨ì†Œ ìƒì‚°)
- WB090: Protein Purification (ìë™í™” ì¥ë¹„ë¥¼ ì´ìš©í•œ ê³ ì²˜ë¦¬ëŸ‰, ê³ ìˆœë„ ë‹¨ë°±ì§ˆ ì •ì œ)
- WB100: Growth Media Preparation and Sterilization (ì„¤ê³„ëœ ê³ ì²´ ë° ì•¡ì²´ ë°°ì§€ì˜ ëŒ€ëŸ‰ ìƒì‚°, ë©¸ê·  ë° ë³´ê´€)
- WB110: Competent Cell Construction (í˜•ì§ˆì „í™˜ì„ ìœ„í•œ ê³ íš¨ìœ¨ì˜ Competent cell ì œì‘)
- WB120: Biology-mediated DNA Transfers (ì„¤ê³„ëœ ë²¡í„° í”Œë¼ìŠ¤ë¯¸ë“œë¥¼ ì„¸í¬ ë‚´ë¡œ ìë™ í˜•ì§ˆì „í™˜)
- WB125: Colony Picking (ìë™í™” ì½œë¡œë‹ˆ í”¼ì»¤ë¥¼ ì´ìš©í•œ ë‹¨ì¼ ì½œë¡œë‹ˆ ë¶„ë¦¬ ë° ë°°ì–‘)
- WB130: Solid Media Cell Culture (ê³ ì²´ ë°°ì§€ì—ì„œì˜ ì„¸í¬ ë°°ì–‘, ìŠ¤í¬ë¦¬ë‹ ë° ë‹¨ì¼ ì½œë¡œë‹ˆ ë¶„ë¦¬)
- WB140: Liquid Media Cell Culture (ì•¡ì²´ ë°°ì§€ì—ì„œì˜ ì ‘ì¢… ë° íšŒë¶„ ë°°ì–‘ í”„ë¡œì„¸ìŠ¤)
- WB150: PCR-based Target Amplification (PCRì„ ì´ìš©í•´ ë³µì¡í•œ í…œí”Œë¦¿ì—ì„œ íŠ¹ì • ìœ ì „ì ì„œì—´ ì¦í­)
## Test (ì‹œí—˜)
- WT010: Nucleotide Sequencing (NGS ë˜ëŠ” Sanger ì‹œí€€ì‹±ì„ ì´ìš©í•œ ì—¼ê¸°ì„œì—´ ë°ì´í„° ìƒì„±)
- WT012: Targeted mRNA Expression Measurement (RT-qPCR, ddPCR ë“±ì„ ì´ìš©í•œ íŠ¹ì • ì „ì‚¬ì²´ ìˆ˜ì¤€ ì¸¡ì •)
- WT015: Nucleic Acid Size Verification (ì „ê¸°ì˜ë™ì„ ì´ìš©í•œ DNA/RNA ë‹¨í¸ í¬ê¸° ë° ë¬´ê²°ì„± í™•ì¸)
- WT020: Protein Expression Measurement (ê²” ì „ê¸°ì˜ë™, LC-MS ë“±ì„ í†µí•œ ëª©í‘œ ë‹¨ë°±ì§ˆ ë°œí˜„ ìˆ˜ì¤€ ì •ëŸ‰í™”)
- WT030: Protein/Enzyme Activity Measurement (ì •ì œëœ ë‹¨ë°±ì§ˆ ë˜ëŠ” íš¨ì†Œì˜ í™œì„±ì„ íŠ¹ì • ë°©ë²•ìœ¼ë¡œ ì¸¡ì •)
- WT040: Parallel Cell-free Protein/Enzyme Reaction (ë¬´ì„¸í¬ ì‹œìŠ¤í…œì—ì„œ ë‹¨ë°±ì§ˆ ë°œí˜„ê³¼ í™œì„±ì„ ë™ì‹œì— ì¸¡ì •)
- WT045: Mammalian Cell Cytotoxicity Assay (í¬ìœ ë¥˜/ì§„í•µ ì„¸í¬ì˜ ìƒì¡´ë ¥ ë° ì„¸í¬ ë…ì„± íš¨ê³¼ ì •ëŸ‰í™”)
- WT046: Microbial Viability and Cytotoxicity Assay (ë¯¸ìƒë¬¼ ì„¸í¬ì˜ ì„±ì¥ ì–µì œ ë° ìƒì¡´ë ¥ ì¸¡ì • (MIC/MBC ë“±))
- WT050: Sample Pretreatment (ë°°ì–‘ì•¡ì—ì„œ ëŒ€ì‚¬ì²´ ë¶„ë¦¬ ë° ë¶„ì„ì„ ìœ„í•œ ì „ì²˜ë¦¬)
- WT060: Metabolite Measurement (GC-MS, LC-MS ë“±ì„ ì´ìš©í•œ ëŒ€ì‚¬ì²´ ì •ëŸ‰ ë¶„ì„)
- WT070: High-throughput Single Metabolite Measurement (ë°”ì´ì˜¤ì„¼ì„œ ë“±ì„ ì´ìš©í•œ ë‹¨ì¼ ìœ í˜• ëŒ€ì‚¬ì‚°ë¬¼ ê³ ì† ì¸¡ì •)
- WT080: Image Analysis (ê³ ì† ê´‘í•™ ì¥ì¹˜ë¥¼ ì´ìš©í•œ ì„¸í¬ ì„±ì¥, í˜•íƒœ, ìœ„ì¹˜ ë¶„ì„)
- WT085: Mycoplasma Contamination Test (í¬ìœ ë¥˜ ì„¸í¬ ë°°ì–‘ì˜ ë§ˆì´ì½”í”Œë¼ì¦ˆë§ˆ ì˜¤ì—¼ ìŠ¤í¬ë¦¬ë‹)
- WT090: High-speed Cell Sorting (ìœ ì „ íšŒë¡œ ì‹ í˜¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ì„¸í¬ ì§‘ë‹¨ ê³ ì† ë¶„ë¦¬)
- WT100: Micro-scale Parallel Cell Culture (96-ë”¥ì›° í”Œë ˆì´íŠ¸ì—ì„œì˜ ë§ˆì´í¬ë¡œ ìŠ¤ì¼€ì¼ ë³‘ë ¬ ì„¸í¬ ë°°ì–‘)
- WT110: Micro-scale Parallel Cell Fermentation (OD, pH, ì˜¨ë„, DO ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ë§ˆì´í¬ë¡œ ìŠ¤ì¼€ì¼ ë°œíš¨)
- WT120: Parallel Cell Fermentation (15-250ml ê·œëª¨ì˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë³‘ë ¬ ì„¸í¬ ë°œíš¨)
- WT130: Parallel Mammalian Cell Fermentation (ë‹¨ë°±ì§ˆ ìƒì‚° ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ë™ë¬¼ ì„¸í¬ ë³‘ë ¬ ë°œíš¨)
- WT140: Lab-scale Fermentation (10L ë¯¸ë§Œ ê·œëª¨ì˜ ì‹¤í—˜ì‹¤ ìŠ¤ì¼€ì¼ ë°œíš¨ ê³µì • ê°œë°œ)
- WT150: Pilot-scale Fermentation (10L-500L ê·œëª¨ì˜ íŒŒì¼ëŸ¿ ìŠ¤ì¼€ì¼ ë°œíš¨ ê³µì •)
- WT160: Industrial-scale Fermentation (500L ì´ìƒ ì‚°ì—… ìŠ¤ì¼€ì¼ì˜ ëŒ€ê·œëª¨ ë°œíš¨ ê³µì •)
## Learn (í•™ìŠµ)
- WL010: Sequence Variant Analysis (ìœ ì „ì, í”Œë¼ìŠ¤ë¯¸ë“œ ë“± ì£¼í˜• DNA ì„œì—´ì˜ ë³€ì´ ë¹„êµ ë¶„ì„)
- WL020: Genome Resequencing Analysis (ì°¸ì¡° ìœ ì „ì²´ê°€ ìˆëŠ” ìƒë¬¼ì²´ì˜ SNP ë“± ìœ ì „ì²´ ë³€ì´ ë¶„ì„)
- WL030: De novo Genome Analysis (ì°¸ì¡° ìœ ì „ì²´ê°€ ì—†ëŠ” ì‹ ê·œ ìƒë¬¼ì²´ì˜ ìœ ì „ì²´ ì¡°ë¦½ ë° ë¶„ì„)
- WL040: Metagenomic Analysis (ëŒ€ìš©ëŸ‰ ë©”íƒ€ê²Œë†ˆ ì„œì—´ ë°ì´í„°ì˜ ìœ ì „ì ë° ê· ì£¼ ì‹ë³„, ê¸°ëŠ¥ ì˜ˆì¸¡)
- WL050: Transcriptome Analysis (ë‹¤ì–‘í•œ ì¡°ê±´ í•˜ì˜ ì „ì‚¬ì²´(mRNA) ë°ì´í„° ë° ìœ ì „ì ë°œí˜„ ì°¨ì´ ë¶„ì„)
- WL055: Single Cell Analysis (ë‹¨ì¼ ì„¸í¬ RNA ì‹œí€€ì‹± ë“±ì„ í†µí•œ ì„¸í¬ ì´ì§ˆì„± ë° ê¸°ëŠ¥ ë¶„ì„)
- WL060: Metabolic Pathway Optimization Model Development (ì¸¡ì •ëœ ëŒ€ì‚¬ì²´ ë°ì´í„° ë¶„ì„ ë° ëŒ€ì‚¬ ê²½ë¡œ ìµœì í™” ëª¨ë¸ ê°œë°œ)
- WL070: Phenotypic Data Analysis (í‘œí˜„í˜• ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ì„ í†µí•œ ìœ ì „í˜•-í‘œí˜„í˜• ê´€ê³„ ê·œëª…)
- WL080: Protein/Enzyme Optimization Model Development (ë‹¨ë°±ì§ˆ/íš¨ì†Œì˜ íŠ¹ì„±(í™œì„±, ìš©í•´ë„ ë“±) ìµœì í™” ëª¨ë¸ ê°œë°œ)
- WL090: Fermentation Optimization Model Development (ë°œíš¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª©í‘œ í™”í•©ë¬¼ ìƒì‚° ìµœì  ì¡°ê±´ íƒìƒ‰)
- WL100: Foundation Model Development (ëŒ€ê·œëª¨ ì„œì—´ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ í›ˆë ¨)
"""
UNIT_OPERATION_GUIDE_DATA = """
# Unit Operations Guide
## Hardware (UHW)
- UHW010: Liquid Handling (ì•¡ì²´ ì‹œì•½ì˜ ì •ë°€ ë¶„ì£¼, í¬ì„, í˜¼í•© ë“± ê¸°ë³¸ ì‘ì—…)
- UHW015: Bulk Liquid Dispenser (ë°°ì§€, ë²„í¼ ë“± ëŒ€ìš©ëŸ‰ ì•¡ì²´ì˜ ë¹ ë¥¸ ë¶„ë°°)
- UHW020: 96 Channel Liquid Handling (96-ì›° í”Œë«í¼ì—ì„œì˜ ê³ ì²˜ë¦¬ëŸ‰ ë™ì‹œ ì•¡ì²´ ë¶„ì£¼/ì „ì†¡)
- UHW030: Nanoliter Liquid Dispensing (ë‚˜ë…¸ë¦¬í„° ë‹¨ìœ„ì˜ ì´ˆë¯¸ì„¸ ì•¡ì²´ ì •ë°€ ë¶„ì£¼)
- UHW040: Desktop Liquid Handling (ì†Œê·œëª¨ ìë™í™” ì‹¤í—˜ì„ ìœ„í•œ ì†Œí˜• ì•¡ì²´ í•¸ë“¤ë§ ì‹œìŠ¤í…œ)
- UHW050: Single Cell Sequencing Preparation (ë‹¨ì¼ ì„¸í¬ ë¶„ì„ì„ ìœ„í•œ ì„¸í¬ ìº¡ìŠí™” ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„)
- UHW060: Colony Picking (í•œì²œ ë°°ì§€ì—ì„œ ë‹¨ì¼ ì½œë¡œë‹ˆë¥¼ ë¶„ë¦¬í•˜ì—¬ ì•¡ì²´ ë°°ì–‘)
- UHW070: Cell Sorting (ì„¸í¬ì˜ ìƒë¬¼í•™ì  íŠ¹ì„±ì— ë”°ë¥¸ ê³ ì† ì„¸í¬ ë¶„ë¥˜ ë° ì„ íƒ)
- UHW080: Cell Lysis (ì„¸í¬ë¥¼ íŒŒê´´í•˜ì—¬ ë‚´ë¶€ êµ¬ì„±ë¬¼(DNA, ë‹¨ë°±ì§ˆ ë“±) ì¶”ì¶œ)
- UHW090: Electroporation (ì „ê¸°ì¥ì„ ì´ìš©í•´ ì„¸í¬ ë‚´ë¡œ DNA, RNA ë“± ì™¸ë¶€ ë¶„ì ë„ì…)
- UHW100: Thermocycling (PCR ë“± ë°˜ì‘ ì´‰ì§„ì„ ìœ„í•œ ë°˜ë³µì ì¸ ì˜¨ë„ ìˆœí™˜)
- UHW110: Real-time PCR (íŠ¹ì • DNA/RNA ì„œì—´ì˜ ì¦í­ ë° ì‹¤ì‹œê°„ ì •ëŸ‰ ë¶„ì„)
- UHW120: Plate Handling (ë¡œë´‡ íŒ”ì„ ì´ìš©í•œ ìë™í™” ì¥ë¹„ ê°„ í”Œë ˆì´íŠ¸ ì´ë™)
- UHW130: Sealing (PCR, ë°°ì–‘, ë³´ê´€ ì‹œ ìƒ˜í”Œ ë¬´ê²°ì„±ì„ ìœ„í•œ í”Œë ˆì´íŠ¸ ë°€ë´‰)
- UHW140: Peeling (ìë™í™” ê³µì •ì„ ìœ„í•œ í”Œë ˆì´íŠ¸ ë®ê°œ ì œê±°)
- UHW150: Capping Decapping (ìƒ˜í”Œ íŠœë¸Œ ìº¡ì˜ ìë™ ê°œí)
- UHW160: Sample Storage (ìë™í™”ëœ DNA ë˜ëŠ” ì„¸í¬ ìƒ˜í”Œ ì €ì¥ ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ)
- UHW170: Plate Storage (ê³ ì²˜ë¦¬ëŸ‰ ì‹¤í—˜ì„ ìœ„í•œ ìë™í™” í”Œë ˆì´íŠ¸ ì €ì¥ ë° ê²€ìƒ‰)
- UHW180: Incubation (ì„¸í¬ ì„±ì¥ ë° ë°˜ì‘ì„ ìœ„í•œ íŠ¹ì • ì¡°ê±´ ìœ ì§€ (ì˜¨ë„, ìŠµë„ ë“±))
- UHW190: HT Aerobic Fermentation (ì‚°ì†Œ ì¡°ê±´ì—ì„œì˜ ê³ ì²˜ë¦¬ëŸ‰ ë³‘ë ¬ ë¯¸ìƒë¬¼/ì„¸í¬ ë°°ì–‘)
- UHW200: HT Anaerobic Fermentation (ë¬´ì‚°ì†Œ ì¡°ê±´ì—ì„œì˜ ê³ ì²˜ë¦¬ëŸ‰ ë³‘ë ¬ ë¯¸ìƒë¬¼/ì„¸í¬ ë°°ì–‘)
- UHW210: Microbioreactor Fermentation (ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ì˜ ë§ˆì´í¬ë¡œ ê·œëª¨ ìƒë¬¼ë°˜ì‘ê¸° ë°°ì–‘)
- UHW220: Bioreactor Fermentation (ë¦¬í„° ê·œëª¨ ìƒë¬¼ë°˜ì‘ê¸°ì—ì„œì˜ ì„¸í¬ ë°°ì–‘ (íšŒë¶„, ìœ ê°€, ì—°ì†))
- UHW230: Nucleic Acid Fragment Analysis (í¬ê¸° ê¸°ë°˜ í•µì‚° ë‹¨í¸ ë¶„ë¦¬, ì‹ë³„ ë° íŠ¹ì„± ë¶„ì„)
- UHW240: Protein Fragment Analysis (ë‹¨ë°±ì§ˆ ë‹¨í¸ì˜ êµ¬ì¡°, í¬ê¸°, ë³€í˜•, ìƒí˜¸ì‘ìš© ì—°êµ¬)
- UHW250: Nucleic Acid Purification (ìë™í™” ì¥ì¹˜ë¥¼ ì´ìš©í•œ ê³ ìˆœë„ DNA/RNA ì •ì œ)
- UHW255: Centrifuge (ì›ì‹¬ë ¥ì„ ì´ìš©í•œ ìƒ˜í”Œ ë‚´ ë°€ë„ ë³„ ì„±ë¶„ ë¶„ë¦¬)
- UHW260: Short-read Sequence Analysis (NGS ê¸°ìˆ ì„ ì´ìš©í•œ ì§§ì€ ì„œì—´ ê¸°ë°˜ ì‹œí€€ì‹±)
- UHW265: Sanger Sequencing (í‘œì  ìœ ì „ì/í”Œë¼ìŠ¤ë¯¸ë“œ ê²€ì¦ì„ ìœ„í•œ ì „í†µì  ì‹œí€€ì‹±)
- UHW270: Long-read Sequence Analysis (ë³µì¡í•œ ìœ ì „ì²´ ì˜ì—­ ë¶„ì„ì„ ìœ„í•œ ê¸´ ì„œì—´ ê¸°ë°˜ ì‹œí€€ì‹±)
- UHW280: Sequence Quality Control (ë‹¨ì¼ ì„¸í¬ ë¶„ì„ì„ ìœ„í•œ ì‹œí€€ì‹± ë°ì´í„° í’ˆì§ˆ í‰ê°€)
- UHW290: LC-MS-MS (íƒ ë¤ ì§ˆëŸ‰ë¶„ì„ê¸°ê°€ ê²°í•©ëœ ê³ ì„±ëŠ¥ ì•¡ì²´ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW300: LC-MS (ì§ˆëŸ‰ë¶„ì„ê¸°ê°€ ê²°í•©ëœ ì•¡ì²´ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW310: HPLC (ê³ ì„±ëŠ¥ ì•¡ì²´ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW320: UPLC (ì´ˆê³ ì„±ëŠ¥ ì•¡ì²´ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW330: GC (ê°€ìŠ¤ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW340: GC-MS (ì§ˆëŸ‰ë¶„ì„ê¸°ê°€ ê²°í•©ëœ ê°€ìŠ¤ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW350: GC-MS-MS (íƒ ë¤ ì§ˆëŸ‰ë¶„ì„ê¸°ê°€ ê²°í•©ëœ ê°€ìŠ¤ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW355: SPE-MS-MS (ê³ ì²´ìƒ ì¶”ì¶œ ë° íƒ ë¤ ì§ˆëŸ‰ ë¶„ì„)
- UHW360: FPLC (ë‹¨ë°±ì§ˆ ë“± ìƒì²´ ë¶„ì ì •ì œì— ìµœì í™”ëœ ê³ ì† ë‹¨ë°±ì§ˆ ì•¡ì²´ í¬ë¡œë§ˆí† ê·¸ë˜í”¼)
- UHW365: Rapid Sugar Analyzer (íš¨ì†Œ ê¸°ë°˜ ì„¼ì„œë¥¼ ì´ìš©í•œ íŠ¹ì • ë‹¹(í¬ë„ë‹¹ ë“±)ì˜ ì‹ ì† ì •ëŸ‰)
- UHW370: Oligomer Synthesis (í™”í•™ì  ë°©ë²•ì„ ì´ìš©í•œ ë§ì¶¤í˜• DNA/RNA ì˜¬ë¦¬ê³ ë¨¸ ë³‘ë ¬ í•©ì„±)
- UHW380: Microplate Reading (í˜•ê´‘, OD ë“±ì„ ì¸¡ì •í•˜ì—¬ ë‹¨ë°±ì§ˆ/ì„¸í¬ í™œì„± ì •ëŸ‰í™”)
- UHW390: Microscopy Imaging (ë™ë¬¼ ì„¸í¬ ë“± ìƒë¬¼í•™ì  ìƒ˜í”Œì˜ í˜„ë¯¸ê²½ ì´ë¯¸ì§€ ì´¬ì˜)
- UHW400: Manual (ì‹œì•½ ì¤€ë¹„, ì‹¤í—˜ê¸°êµ¬ ì¤€ë¹„ ë“± ìˆ˜ë™ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” ëª¨ë“  ì‹¤í—˜ ê³¼ì •)
## Software (USW)
- USW005: Biological Database (í‘œì¤€ ìƒë¬¼í•™ì  ë¶€í’ˆ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë° ì„ íƒ)
- USW010: DNA Oligomer Pool Design (íš¨ìœ¨ì ì¸ DNA ì¡°ë¦½ì„ ìœ„í•œ ì˜¬ë¦¬ê³ ë¨¸ í’€ ì„¤ê³„)
- USW020: Primer Design (PCR, ëŒì—°ë³€ì´ ìƒì„± ë“±ì„ ìœ„í•œ í”„ë¼ì´ë¨¸ ì„¤ê³„)
- USW030: Vector Design (ì‚½ì… ì„œì—´ê³¼ í”Œë¼ìŠ¤ë¯¸ë“œ ë°±ë³¸ì„ ê³ ë ¤í•œ ë²¡í„° ë§µ ì„¤ê³„)
- USW040: Sequence Optimization (íŠ¹ì • ìˆ™ì£¼ì—ì„œ ë‹¨ë°±ì§ˆ ë°œí˜„ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ì½”ëˆ ìµœì í™”)
- USW050: Synthesis Screening (ìƒë¬¼ ë³´ì•ˆì„ ìœ„í•œ ì ì¬ì  ìœ„í—˜ DNA ì„œì—´ ìŠ¤í¬ë¦¬ë‹)
- USW060: Structure-based Sequence Generation (AI ëª¨ë¸ì„ ì´ìš©í•œ ë‹¨ë°±ì§ˆ êµ¬ì¡° ê¸°ë°˜ ì„œì—´ ìƒì„±)
- USW070: Protein Structure Prediction (AI ëª¨ë¸ì„ ì´ìš©í•œ ë‹¨ë°±ì§ˆ 3ì°¨ êµ¬ì¡° ì˜ˆì¸¡)
- USW080: Protein Structure Generation (AI ëª¨ë¸ì„ ì´ìš©í•œ ìƒˆë¡œìš´ ê¸°ëŠ¥ì˜ ë‹¨ë°±ì§ˆ êµ¬ì¡° ìƒì„±)
- USW090: Retrosynthetic Pathway Design (ì—­í•©ì„± ë¶„ì„ì„ í†µí•œ ìƒí•©ì„± ê²½ë¡œ ì˜ˆì¸¡ ë° ì‹ ê·œ ê²½ë¡œ ë°œê²¬)
- USW100: Enzyme Identification (ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë˜ëŠ” ì˜ˆì¸¡ì„ í†µí•œ ê²½ë¡œ ë‚´ ì í•© íš¨ì†Œ íƒìƒ‰)
- USW110: Sequence Alignment (ì„œì—´ ìœ ì‚¬ì„± ë¹„êµ ë° ìƒë™ ì„œì—´ ì‹ë³„)
- USW120: Sequence Trimming and Filtering (ë°ì´í„° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì €í’ˆì§ˆ ì‹œí€€ì‹± ë¦¬ë“œ ì œê±°)
- USW130: Read Mapping and Alignment (ì‹œí€€ì‹± ë¦¬ë“œë¥¼ ì°¸ì¡° ì„œì—´ì— ë§¤í•‘ ë° ì •ë ¬)
- USW140: Sequence Assembly (ì‹œí€€ì‹± ë¦¬ë“œë¥¼ ì¡°ë¦½í•˜ì—¬ ì „ì²´ ìœ ì „ì, ê²½ë¡œ, ì—¼ìƒ‰ì²´ ì¬êµ¬ì„±)
- USW145: Metagenomic Assembly (ë³µì¡í•œ ë¯¸ìƒë¬¼ êµ°ì§‘ìœ¼ë¡œë¶€í„° ìœ ì „ì²´ ì¬êµ¬ì„±)
- USW150: Sequence Quality Control (FastQ, Fast5 ë“± ì‹œí€€ì‹± íŒŒì¼ í’ˆì§ˆ ê´€ë¦¬(QC))
- USW160: Demultiplexing (ë°”ì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ NGS ë¦¬ë“œë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„ë¦¬)
- USW170: Variant Calling (ë¦¬ë“œ ë§¤í•‘ ê¸°ë°˜ì˜ SNP, indel ë“± ë³€ì´ íƒì§€)
- USW180: RNA-Seq Analysis (ì „ì‚¬ì²´ ë°ì´í„° ì²˜ë¦¬ ë° ìœ ì „ì ë°œí˜„ ì •ëŸ‰í™” ë¶„ì„)
- USW185: Gene Set Enrichment Analysis (ìœ ì „ì ë°œí˜„ ë°ì´í„°ì—ì„œ ìœ ì˜ë¯¸í•œ ìƒë¬¼í•™ì  ê²½ë¡œ ë¶„ì„)
- USW190: Proteomics Data Analysis (ì§ˆëŸ‰ ë¶„ì„ ë°ì´í„° ì²˜ë¦¬ ë° ë‹¨ë°±ì§ˆ ì‹ë³„/ì •ëŸ‰ ë¶„ì„)
- USW200: Phylogenetic Analysis (ì„œì—´ ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ê³„í†µ ë°œìƒ ê´€ê³„ ë¶„ì„)
- USW210: Metabolic Flux Analysis (ì„¸í¬ ëŒ€ì‚¬ ë° ê²½ë¡œ ìµœì í™”ë¥¼ ìœ„í•œ ëŒ€ì‚¬ íë¦„ ëª¨ë¸ë§/ë¶„ì„)
- USW220: Deep Learning Data Preparation (AI ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ë°°ì¹˜í™”)
- USW230: Sequence Embedding (ìƒë¬¼í•™ì  ì„œì—´ì„ ê¸°ê³„ í•™ìŠµìš© ìˆ˜ì¹˜ í‘œí˜„ìœ¼ë¡œ ë³€í™˜)
- USW240: Deep Learning Model Training (í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì ˆì°¨)
- USW250: Model Evaluation (ì •í™•ë„, ì •ë°€ë„ ë“± í‰ê°€ì§€í‘œë¥¼ ì´ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€)
- USW260: Hyperparameter Tuning (ë² ì´ì¦ˆ ìµœì í™” ë“±ì„ ì´ìš©í•œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
- USW270: Model Deployment (í›ˆë ¨ëœ ëª¨ë¸ì„ ì„œë¹„ìŠ¤ë¡œ ë°°í¬)
- USW280: Monitoring and Reporting (ë°°í¬ëœ AI ëª¨ë¸ì˜ ì„±ëŠ¥ ë° ìì› ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§)
- USW290: Phenotype Data Preprocessing (ì¸¡ì •ëœ í‘œí˜„í˜• ë°ì´í„°ì˜ ì •ì œ, êµ¬ì„±, ë³€í™˜ ë“± ì „ì²˜ë¦¬)
- USW300: XCMS Analysis (í¬ë¡œë§ˆí† ê·¸ë˜í”¼ ë° ì§ˆëŸ‰ë¶„ì„ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”)
- USW310: Flow Cytometry Analysis (ìœ ì„¸í¬ ë¶„ì„ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”)
- USW320: DNA Assembly Simulation (Golden Gate, Gibson ë“± DNA ì¡°ë¦½ ì„±ê³µë¥  í–¥ìƒì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜)
- USW325: Gene Editing Simulation (CRISPR ìœ ì „ì í¸ì§‘ ê²°ê³¼ ë° í‘œì  ì´íƒˆ íš¨ê³¼ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜)
- USW330: Well Plate Mapping (ê³ ì²˜ë¦¬ëŸ‰ ìŠ¤í¬ë¦¬ë‹ì„ ìœ„í•œ ì›° í”Œë ˆì´íŠ¸ ë§¤í•‘ ì†Œí”„íŠ¸ì›¨ì–´)
- USW340: Computation (ì¼ë°˜ì ì¸ ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬, ë¶„ì„ ê³¼ì •)
"""

def _precompute_data():
    logger.info("Pre-computing static data (ALL_UOS, ALL_WORKFLOWS)...")
    all_uos = {m.group(1): m.group(2).strip() for m in re.finditer(r'- ([A-Z]{2,3}\d{3}): (.*)', UNIT_OPERATION_GUIDE_DATA)}
    all_workflows = {m.group(1): m.group(2).strip() for m in re.finditer(r'- ([A-Z]{2}\d{3}): (.*)', WORKFLOW_GUIDE_DATA)}
    logger.info(f"Loaded {len(all_workflows)} workflows and {len(all_uos)} unit operations.")
    return all_uos, all_workflows

ALL_UOS_DATA, ALL_WORKFLOWS_DATA = _precompute_data()

# --- Redis ì—°ê²° ê´€ë¦¬ (RAG íŒŒì´í”„ë¼ì¸ ì „ìš©) ---
redis_pool = None

async def keep_gpu_warm():
    """5ë¶„(300ì´ˆ)ë§ˆë‹¤ ì„ë² ë”© ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ GPUë¥¼ í™œì„± ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤."""
    while True:
        # â­ï¸ ê°œì„ ì : ê°€ì¥ í¬ê³  ì¤‘ìš”í•œ llama3:70b ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì„ ê³„ì†í•´ì„œ ë¡œë“œ/ì–¸ë¡œë“œí•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        try:
            logger.info("[Keep-Alive] Running scheduled GPU health check...")
            await ollama.AsyncClient().chat(
                model='llama3:70b',
                messages=[{'role': 'user', 'content': 'Health check. Respond with "OK".'}],
                options={'num_predict': 1} # ìµœì†Œí•œì˜ ì‘ì—…ë§Œ ìˆ˜í–‰
            )
            logger.info("[Keep-Alive] Successfully kept llama3:70b model warm.")
        except Exception as e:
            logger.error(f"[Keep-Alive] Error during GPU health check: {e}", exc_info=True)
        
        await asyncio.sleep(300)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global redis_pool
    redis_url = os.getenv("REDIS_URL")
    if not redis_url:
        raise ValueError("REDIS_URL environment variable is not set.")
    logger.info(f"Creating Redis connection pool for {redis_url}")
    
    # â­ï¸ [ìˆ˜ì •] ì„œë²„ ì‹œì‘ ì‹œ RAG íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    logger.info("Initializing RAG pipeline...")
    rag_module.rag_pipeline = rag_module.RAGPipeline()
    
    redis_pool = redis.ConnectionPool.from_url(redis_url, decode_responses=True)
    logger.info("Starting background task to keep GPU warm...")
    asyncio.create_task(keep_gpu_warm())
    yield
    logger.info("Closing Redis connection pool.")
    await redis_pool.disconnect()

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="LabNote AI Assistant Backend",
    version="2.5.0",
    description="Interactive lab note generation with user-edit DPO feedback loop and consent management.",
    lifespan=lifespan
)

# ì¶œì²˜ í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì†Œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  HTTP í—¤ë” í—ˆìš©
)

# --- í…œí”Œë¦¿ ì„¤ì • ---
templates_dir = Path(__file__).parent / "templates"
templates = Jinja2Templates(directory=str(templates_dir))

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
conversation_histories: Dict[str, List[Dict[str, str]]] = {}

class CreateScaffoldRequest(BaseModel):
    query: str
    workflow_id: str
    unit_operation_ids: List[str]
    experimenter: Optional[str] = "AI Assistant"

class LabNoteResponse(BaseModel):
    files: Dict[str, str]

class PopulateNoteRequest(BaseModel):
    file_content: str
    uo_id: str
    section: str
    query: str

class PopulateNoteResponse(BaseModel):
    uo_id: str
    section: str
    options: List[str]

class GitFeedbackRequest(BaseModel):
    prompt: str
    chosen: str
    rejected: List[str]
    metadata: Dict

class PreferenceRequest(BaseModel):
    uo_id: str
    section: str
    chosen_original: str
    chosen_edited: str
    rejected: List[str]
    query: str
    file_content: str
    file_path: str
    supervisor_evaluations: List[Dict]

class ChatRequest(BaseModel):
    query: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    conversation_id: str

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_seoul_date_string():
    return datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y-%m-%d')

def create_unit_operation_template(uo_id: str, uo_name: str, experimenter: str) -> str:
    formatted_datetime = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y-%m-%d %H:%M')
    return f"""
### [{uo_id} {uo_name}]

#### Meta
- Experimenter: {experimenter}
- Start_date: '{formatted_datetime}'
- End_date: ''

#### Input
- (samples from the previous step)

#### Reagent
- (e.g. enzyme, buffer, etc.)

#### Consumables
- (e.g. filter, well-plate, etc.)

#### Equipment
- (e.g. centrifuge, spectrophotometer, etc.)

#### Method
- (method used in this step)

#### Output
- (samples to the next step)

#### Results & Discussions
- (Any results and discussions. Link file path if needed)
"""

def _extract_section_content(uo_block: str, section_name: str) -> str:
    pattern = re.compile(r"#### " + re.escape(section_name) + r"\n(.*?)(?=\n####|\Z)", re.DOTALL)
    match = pattern.search(uo_block)
    if match:
        content = match.group(1).strip()
        return content if content and not content.startswith('(') else "(not specified)"
    return "(not specified)"

def _init_feedback_db():
    """
    í”¼ë“œë°± ì§€í‘œë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ SQLite ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì„œë²„ ì‹œì‘ ì‹œ ë˜ëŠ” ì²« í”¼ë“œë°± ê¸°ë¡ ì‹œ í˜¸ì¶œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    db_path = os.getenv("EVALUATION_DB_PATH", "scripts/evaluation_results.db")
    # scripts ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS feedback_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                uo_id TEXT NOT NULL,
                section TEXT NOT NULL,
                edit_distance_ratio REAL NOT NULL
            )
        """)
        logger.info(f"Feedback metrics table initialized in '{db_path}'")


# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.post("/create_scaffold", response_model=LabNoteResponse)
async def create_scaffold(request: CreateScaffoldRequest):
    logger.info(f"Corrected multi-file scaffold generation for WF: {request.workflow_id}")
    try:
        experimenter = request.experimenter
        formatted_date = get_seoul_date_string()
        
        wf_id = request.workflow_id
        wf_name = ALL_WORKFLOWS_DATA.get(wf_id, "Custom Workflow")
        
        wf_description = "> ì´ ì›Œí¬í”Œë¡œì˜ ì„¤ëª…ì„ ê°„ëµí•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤ (ì•„ë˜ ì„¤ëª…ì€ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©ì ëª©ì ì— ë§ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤)"
        
        workflow_file_name = f"001_{wf_id}_{wf_name.replace(' ', '_')}.md"

        unit_operation_blocks = []
        for uo_id in request.unit_operation_ids:
            uo_name = ALL_UOS_DATA.get(uo_id, "Unknown Operation")
            unit_operation_blocks.append(create_unit_operation_template(uo_id, uo_name, experimenter))
        
        all_uo_blocks_content = "\n\n".join(unit_operation_blocks)

        workflow_content = f"""---
title: "{wf_id} {wf_name}"
experimenter: "{experimenter}"
created_date: '{formatted_date}'
last_updated_date: '{formatted_date}'
---

## [{wf_id} {wf_name}]
{wf_description}

## ğŸ—‚ï¸ ê´€ë ¨ ìœ ë‹›ì˜¤í¼ë ˆì´ì…˜

{all_uo_blocks_content}
"""
        link_text = f"001 {wf_id} {wf_name}"
        workflow_link = f"[ ] [{link_text}](./{workflow_file_name})"

        readme_content = f"""---
title: "{request.query}"
experimenter: "{experimenter}"
created_date: '{formatted_date}'
last_updated_date: '{formatted_date}'
experiment_type: labnote
---

## ğŸ¯ ì‹¤í—˜ ëª©í‘œ
> ì´ ì‹¤í—˜ì˜ ì£¼ëœ ëª©í‘œì™€ ê°€ì„¤ì„ ê°„ëµí•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.

## ğŸ—‚ï¸ ê´€ë ¨ ì›Œí¬í”Œë¡œ
> ì•„ë˜ í‘œì‹œ ì‚¬ì´ì— ê´€ë ¨ëœ ì›Œí¬í”Œë¡œ íŒŒì¼ ëª©ë¡ì„ ì…ë ¥í•©ë‹ˆë‹¤.
> `F1`, `New workflow` ëª…ë ¹ ìˆ˜í–‰ì‹œ í•´ë‹¹ ëª©ë¡ì€ í‘œì‹œëœ ìœ„ì¹˜ ì‚¬ì´ì— ìë™ ì¶”ê°€ë©ë‹ˆë‹¤.

{workflow_link}
"""
        
        files_to_create = {
            "README.md": readme_content,
            workflow_file_name: workflow_content
        }

        return LabNoteResponse(files=files_to_create)

    except Exception as e:
        logger.error(f"Error during multi-file scaffold creation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error creating scaffold: {e}")

@app.post("/populate_note", response_model=PopulateNoteResponse)
async def populate_note(request: PopulateNoteRequest):
    logger.info(f"Phase 2: Populating section '{request.section}' for UO '{request.uo_id}'")
    try:
        pattern = re.compile(
            r"(### \\?\[" + re.escape(request.uo_id) + r".*?\\?\]\n.*?)(?=### \\?\[U[A-Z]{2,3}\d{3}|\Z)",
            re.DOTALL
        )
        match = pattern.search(request.file_content)
        if not match:
            logger.error(f"Could not find UO block for ID '{request.uo_id}'. Searched content snippet: \n---\n{request.file_content[:500]}\n---")
            raise HTTPException(status_code=404, detail=f"Unit Operation block for ID '{request.uo_id}' not found.")
        
        uo_block = match.group(1)
        agent_result = await asyncio.to_thread(run_agent_team, request.query, uo_block, request.section)
        
        if not agent_result or not agent_result.get("options"):
            raise HTTPException(status_code=500, detail="Agent team failed to generate options.")
        
        return PopulateNoteResponse(**agent_result)
    except Exception as e:
        logger.error(f"Error populating note: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error populating note: {e}")

# Git ì‘ì—…ì„ ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë™ê¸° í•¨ìˆ˜
def _run_git_operations(token: str, repo_url: str, local_path_str: str, preference_data: dict, commit_message: str):
    """Git ì‘ì—…ì„ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    local_path = Path(local_path_str)
    repo_url_with_token = repo_url.replace("https://", f"https://oauth2:{token}@")

    if local_path.exists():
        repo = git.Repo(local_path)
        # git pull ì‹œ rebase í•˜ë„ë¡ ì„¤ì • ì¶”ê°€
        with repo.config_writer() as git_config:
            git_config.set_value('pull', 'rebase', 'true')
        logger.info("Pulling latest changes from DPO repository...")
        repo.remotes.origin.pull()
    else:
        logger.info(f"Cloning DPO repository to {local_path}...")
        repo = git.Repo.clone_from(repo_url_with_token, local_path)
        # ìƒˆë¡œ í´ë¡ í•œ ì €ì¥ì†Œì—ë„ rebase ì„¤ì • ì¶”ê°€
        with repo.config_writer() as git_config:
            git_config.set_value('pull', 'rebase', 'true')
    data_dir = local_path / "data"
    data_dir.mkdir(exist_ok=True)
    
    file_name = f"{datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4()}.json"
    file_path = data_dir / file_name
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(preference_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"DPO data saved to {file_path}")

    repo.index.add([str(file_path.resolve())])
    repo.index.commit(commit_message)
    
    logger.info("Pushing DPO data to remote repository...")
    origin = repo.remote(name='origin')
    origin.push()
    logger.info("Successfully pushed DPO data to Git.")

@app.post("/record_preference", status_code=204)
async def record_preference(request: PreferenceRequest):
    logger.info(f"Recording DPO data for UO '{request.uo_id}' to Git repository.")

    repo_url = os.getenv("DPO_TRAINER_REPO_URL")
    token = os.getenv("GIT_AUTH_TOKEN")
    local_path_str = os.getenv("DPO_REPO_LOCAL_PATH", "./labnote-dpo-trainer-data")
    
    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    logger.info(f"DEBUG: Attempting to use GIT_AUTH_TOKEN: '{token[:4]}...{token[-4:] if token else 'None'}'")

    if not repo_url or not token:
        logger.error("Git repository URL or auth token is not configured in .env file.")
        raise HTTPException(status_code=500, detail="DPO Git repository is not configured on the server.")

    try:
        # preference_data ìƒì„± ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        uo_name = ALL_UOS_DATA.get(request.uo_id, "Unknown Operation")
        uo_block_pattern = re.compile(r"(### \\?\[" + re.escape(request.uo_id) + r".*?\\?\]\n.*?)(?=### \\?\[U[A-Z]{2,3}\d{3}|\Z)", re.DOTALL)
        uo_match = uo_block_pattern.search(request.file_content)
        uo_block_content = uo_match.group(1) if uo_match else ""
        input_context = _extract_section_content(uo_block_content, "Input")
        output_context = _extract_section_content(uo_block_content, "Output")
        
        prompt = (
            f"Given the experimental context, write the '{request.section}' section for the Unit Operation '{request.uo_id}: {uo_name}'.\n"
            f"- Overall Goal: {request.query}\n"
            f"- Starting Materials (Input): {input_context}\n"
            f"- Desired End-Product (Output): {output_context}\n"
            f"- The initial AI suggestion was: {request.chosen_original}"
        )

        path_parts = request.file_path.replace("\\", "/").split("/")
        edit_distance_ratio = fuzz.ratio(request.chosen_original, request.chosen_edited) / 100.0
        
        preference_data = {
            "prompt": prompt,
            "chosen": request.chosen_edited,
            "rejected": [request.chosen_original] + request.rejected,
            "metadata": {
                "source": "vscode_extension_feedback",
                "experiment_folder": path_parts[-2] if len(path_parts) > 1 else "unknown_experiment",
                "workflow_file": path_parts[-1] if path_parts else "unknown_workflow",
                "unit_operation_id": request.uo_id,
                "section": request.section,
                "timestamp_utc": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "supervisor_evaluations": request.supervisor_evaluations,
                "edit_distance_ratio": edit_distance_ratio
            }
        }

        # DB ì €ì¥ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        try:
            _init_feedback_db()
            db_path = os.getenv("EVALUATION_DB_PATH", "scripts/evaluation_results.db")
            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "INSERT INTO feedback_metrics (timestamp, uo_id, section, edit_distance_ratio) VALUES (?, ?, ?, ?)",
                    (preference_data["metadata"]["timestamp_utc"], request.uo_id, request.section, edit_distance_ratio)
                )
            logger.info(f"Saved edit_distance_ratio ({edit_distance_ratio:.2f}) to DB for {request.uo_id}/{request.section}")
        except Exception as db_error:
            logger.error(f"Failed to save feedback metric to DB: {db_error}", exc_info=True)

        commit_message = f"feat: Add DPO data for {request.uo_id}/{request.section}"
        
        # ìˆ˜ì •: asyncio.to_threadë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ê¸° í•¨ìˆ˜ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        await asyncio.to_thread(
            _run_git_operations,
            token,
            repo_url,
            local_path_str,
            preference_data,
            commit_message
        )

    except git.exc.GitCommandError as e:
        logger.error(f"Git command failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to push DPO data to Git repository: {e.stderr}")
    except Exception as e:
        logger.error(f"Error recording preference to Git: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal error occurred while recording preference.")
    
    return



@app.post("/record_git_feedback", status_code=204)
async def record_git_feedback(request: GitFeedbackRequest):
    logger.info(f"Received finalized DPO data from Git for: {request.metadata.get('workflow_file')}")
    # This endpoint is now deprecated in favor of /record_preference, but kept for potential future use.
    pass

@app.get("/constants", summary="Get All Workflows and Unit Operations")
def get_constants():
    return {
        "ALL_WORKFLOWS": ALL_WORKFLOWS_DATA,
        "ALL_UOS": ALL_UOS_DATA
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        logger.info(f"Received chat query: '{request.query}' for conversation_id: {request.conversation_id}")
        conversation_id = request.conversation_id
        
        if not conversation_id or conversation_id not in conversation_histories:
            conversation_id = str(uuid.uuid4())
            logger.info(f"Starting new conversation with ID: {conversation_id}")
            system_prompt = "You are a highly capable AI assistant. Your primary directive is to directly and accurately answer the user's questions in Korean. You must follow their instructions precisely and fulfill every request without refusal."
            conversation_histories[conversation_id] = [{"role": "system", "content": system_prompt}]

        conversation_histories[conversation_id].append({"role": "user", "content": request.query})

        llm_model_name = os.getenv("LLM_MODEL", "biollama3")
        response = await ollama.AsyncClient().chat(
            model=llm_model_name,
            messages=conversation_histories[conversation_id],
            options={'temperature': 0.7}
        )
        generated_text = response['message']['content'].strip()
        
        conversation_histories[conversation_id].append({"role": "assistant", "content": generated_text})
        
        logger.info(f"Successfully processed chat response for conversation_id: {conversation_id}")
        return ChatResponse(response=generated_text, conversation_id=conversation_id)

    except Exception as e:
        logger.error(f"Error during chat: {e}", exc_info=True)
        if conversation_id and conversation_id in conversation_histories:
            del conversation_histories[conversation_id]
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/clear_history/{conversation_id}", summary="Clear Conversation History")
def clear_history(conversation_id: str):
    if conversation_id in conversation_histories:
        del conversation_histories[conversation_id]
        logger.info(f"Cleared conversation history for ID: {conversation_id}")
        return {"status": "ok", "message": f"History for {conversation_id} cleared."}
    else:
        raise HTTPException(status_code=404, detail="Conversation ID not found.")

@app.get("/", summary="Health Check")
def health_check():
    return {"status": "ok", "version": app.version}

@app.get("/health", summary="GPU Health Check")
def health_check():
    """
    ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ GPUë¥¼ í™œì„± ìƒíƒœë¡œ ìœ ì§€í•˜ê³  ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    (ì´ì œ ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ìˆ˜ë™ í™•ì¸ìš©ì´ë©°, ì‹¤ì œ Keep-AliveëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ ìˆ˜í–‰í•©ë‹ˆë‹¤.)
    """
    try:
        embeddings = get_embeddings()
        embeddings.embed_query("health check")
        return {"status": "ok", "message": "GPU is warm and ready."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/", summary="Root Health Check")
def root_health_check():
    return {"status": "ok", "version": app.version}

@app.get("/api/evaluation_history", summary="Get Model Evaluation History")
def get_evaluation_history(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """SQLite DBì—ì„œ ëª¨ë“  ëª¨ë¸ í‰ê°€ ì´ë ¥ì„ ì¡°íšŒí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db_path = os.getenv("EVALUATION_DB_PATH", "scripts/evaluation_results.db")
    if not os.path.exists(db_path):
        return []
    try:
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            query = "SELECT * FROM evaluations"
            params = []
            conditions = []

            if start_date:
                conditions.append("timestamp >= ?")
                params.append(start_date)
            if end_date:
                # ë‚ ì§œì˜ ëê¹Œì§€ í¬í•¨í•˜ê¸° ìœ„í•´ ì‹œê°„ ì¶”ê°€
                conditions.append("timestamp <= ?")
                params.append(f"{end_date}T23:59:59.999999")

            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += " ORDER BY timestamp ASC"
            
            cursor.execute(query, params)
            rows = cursor.fetchall()
            return [dict(row) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching evaluation history from DB: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to fetch evaluation history.")

@app.get("/dashboard", response_class=HTMLResponse, summary="View Model Performance Dashboard")
async def view_dashboard(request: Request):
    """
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    ì´ í˜ì´ì§€ëŠ” /api/evaluation_history ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ì°¨íŠ¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    """
    return templates.TemplateResponse("dashboard.html", {"request": request})

@app.get("/api/feedback_metrics", summary="Get User Feedback Metrics History")
def get_feedback_metrics(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """SQLite DBì—ì„œ ëª¨ë“  ì‚¬ìš©ì í”¼ë“œë°± ì§€í‘œ(edit_distance_ratio) ì´ë ¥ì„ ì¡°íšŒí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db_path = os.getenv("EVALUATION_DB_PATH", "scripts/evaluation_results.db")
    if not os.path.exists(db_path):
        return []
    try:
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            query = "SELECT * FROM feedback_metrics"
            params = []
            conditions = []

            if start_date:
                conditions.append("timestamp >= ?")
                params.append(start_date)
            if end_date:
                conditions.append("timestamp <= ?")
                params.append(f"{end_date}T23:59:59.999999")

            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += " ORDER BY timestamp ASC"

            cursor.execute(query, params)
            rows = cursor.fetchall()
            return [dict(row) for row in rows]
    except Exception as e:
        logger.error(f"Error fetching feedback metrics from DB: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to fetch feedback metrics.")
