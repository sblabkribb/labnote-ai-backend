import os
import json
import asyncio
import argparse
import logging
from typing import List, Dict
from dotenv import load_dotenv
import re
import sqlite3
import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ ìœ í‹¸ë¦¬í‹°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from llm_utils import call_llm_api

# --- ì´ˆê¸° ì„¤ì • ---
load_dotenv(dotenv_path='../.env')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ---
DB_PATH = os.getenv("EVALUATION_DB_PATH", "evaluation_results.db")

def init_db():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  evaluations í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                model_a_name TEXT NOT NULL,
                model_b_name TEXT NOT NULL,
                judge_model_name TEXT NOT NULL,
                total_prompts INTEGER NOT NULL,
                win_count_b INTEGER NOT NULL,
                loss_count_b INTEGER NOT NULL,
                tie_count INTEGER NOT NULL,
                error_count INTEGER NOT NULL,
                win_rate_b REAL NOT NULL,
                evaluation_log_path TEXT
            )
        """)
        logger.info(f"Database initialized at '{DB_PATH}'")

def save_evaluation_to_db(summary: Dict):
    """í‰ê°€ ìš”ì•½ ì •ë³´ë¥¼ SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.cursor()
        cursor.execute("INSERT INTO evaluations VALUES (NULL, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", tuple(summary.values()))
        logger.info(f"Evaluation summary for '{summary['model_b_name']}' saved to database.")

# --- ì‹¬íŒ(Judge)ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ---
JUDGE_SYSTEM_PROMPT = "You are a fair and impartial AI evaluator. Your task is to compare two AI-generated responses to a user's prompt and determine which one is better. Your evaluation should be based on helpfulness, accuracy, detail, and adherence to the prompt's instructions. Your output MUST be a valid JSON object."

JUDGE_USER_PROMPT_TEMPLATE = """
Please evaluate the two responses from different AI models (Model A and Model B) for the given user prompt.

[USER PROMPT]
{prompt}

[MODEL A RESPONSE]
{response_a}

[MODEL B RESPONSE]
{response_b}

[TASK]
Compare the two responses and decide which one is better. Your decision should be one of "Model A", "Model B", or "Tie". Provide a brief justification for your choice.

Respond with a JSON object in the following format:
{{
  "winner": "Model A" | "Model B" | "Tie",
  "justification": "Your reasoning here."
}}
"""

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

async def get_model_response(prompt: str, model_name: str) -> str:
    """ì§€ì •ëœ ëª¨ë¸ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    system_prompt = "You are a helpful scientific assistant. Provide a direct and detailed answer to the user's request."
    return await call_llm_api(system_prompt, prompt, model_name)

async def evaluate_pair(prompt: str, response_a: str, response_b: str, judge_model: str) -> Dict:
    """ì‹¬íŒ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì‘ë‹µ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ í‰ê°€í•©ë‹ˆë‹¤."""
    judge_prompt = JUDGE_USER_PROMPT_TEMPLATE.format(
        prompt=prompt,
        response_a=response_a,
        response_b=response_b
    )
    
    evaluation_str = await call_llm_api(JUDGE_SYSTEM_PROMPT, judge_prompt, judge_model)
    
    try:
        # LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë§Œ ì •í™•íˆ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', evaluation_str, re.DOTALL)
        if not json_match:
            raise json.JSONDecodeError("No JSON object found in the LLM response.", evaluation_str, 0)
        
        evaluation = json.loads(json_match.group(0))
        if "winner" not in evaluation or "justification" not in evaluation:
            raise ValueError("Judge response missing 'winner' or 'justification' key.")
        return evaluation
    except (json.JSONDecodeError, ValueError) as e:
        logger.error(f"Failed to parse judge's response. Error: {e}. Response: {evaluation_str}")
        return {"winner": "Error", "justification": f"Parsing failed: {e}"}

async def run_evaluation(model_a: str, model_b: str, judge_model: str, eval_dataset_path: str, output_log_path: str):
    """
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    logger.info(f"ğŸš€ Starting evaluation...")
    logger.info(f"  - Model A (Baseline): {model_a}")
    logger.info(f"  - Model B (Candidate): {model_b}")
    logger.info(f"  - Judge Model: {judge_model}")
    logger.info(f"  - Evaluation Dataset: {eval_dataset_path}")

    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        with open(eval_dataset_path, 'r', encoding='utf-8') as f:
            eval_data = json.load(f)
            if not isinstance(eval_data, list) or not all("prompt" in item for item in eval_data):
                 raise ValueError("Evaluation data must be a JSON list of objects, each with a 'prompt' key.")
    except (FileNotFoundError, json.JSONDecodeError, ValueError) as e:
        logger.error(f"âŒ Failed to load or parse evaluation dataset: {e}")
        return

    # 2. ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ í‰ê°€ ì‹¤í–‰
    results = []
    scores = {"Model A": 0, "Model B": 0, "Tie": 0, "Error": 0}

    for i, item in enumerate(eval_data):
        prompt = item["prompt"]
        logger.info(f"--- Evaluating prompt {i+1}/{len(eval_data)} ---")
        
        # ë‘ ëª¨ë¸ì˜ ì‘ë‹µì„ ë™ì‹œì— ìƒì„±
        response_a, response_b = await asyncio.gather(
            get_model_response(prompt, model_a),
            get_model_response(prompt, model_b)
        )

        # ì‹¬íŒì˜ í‰ê°€ ë°›ê¸°
        evaluation = await evaluate_pair(prompt, response_a, response_b, judge_model)
        
        winner = evaluation.get("winner", "Error")
        scores[winner] = scores.get(winner, 0) + 1
            
        result_entry = {
            "prompt": prompt,
            "model_a_response": response_a,
            "model_b_response": response_b,
            "evaluation": evaluation
        }
        results.append(result_entry)
        
        logger.info(f"  -> Winner: {winner}. Justification: {evaluation.get('justification', 'N/A')}")

    # 3. ê²°ê³¼ ìš”ì•½ ë° ì¶œë ¥
    total_comparisons = len(eval_data)
    win_rate_b = (scores["Model B"] / total_comparisons) * 100 if total_comparisons > 0 else 0
    loss_rate_b = (scores["Model A"] / total_comparisons) * 100 if total_comparisons > 0 else 0
    tie_rate = (scores["Tie"] / total_comparisons) * 100 if total_comparisons > 0 else 0

    logger.info("\n--- ğŸ“Š Evaluation Summary ---")
    logger.info(f"Total Prompts: {total_comparisons}")
    logger.info(f"Model B ('{model_b}') Win Rate: {win_rate_b:.2f}% ({scores['Model B']} wins)")
    logger.info(f"Model B ('{model_b}') Loss Rate: {loss_rate_b:.2f}% ({scores['Model A']} wins)")
    logger.info(f"Tie Rate: {tie_rate:.2f}% ({scores['Tie']} ties)")
    if scores["Error"] > 0:
        logger.warning(f"Evaluation Errors: {scores['Error']}")
    logger.info("--------------------------\n")

    # 4. ìƒì„¸ ë¡œê·¸ íŒŒì¼ ì €ì¥
    try:
        with open(output_log_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… Detailed evaluation log saved to '{output_log_path}'")
    except IOError as e:
        logger.error(f"âŒ Failed to save evaluation log: {e}")

    # 5. ë°ì´í„°ë² ì´ìŠ¤ì— ìš”ì•½ ê²°ê³¼ ì €ì¥
    summary_data = {
        "timestamp": datetime.datetime.now().isoformat(),
        "model_a_name": model_a,
        "model_b_name": model_b,
        "judge_model_name": judge_model,
        "total_prompts": total_comparisons,
        "win_count_b": scores["Model B"],
        "loss_count_b": scores["Model A"],
        "tie_count": scores["Tie"],
        "error_count": scores["Error"],
        "win_rate_b": win_rate_b,
        "evaluation_log_path": output_log_path
    }
    save_evaluation_to_db(summary_data)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate DPO model performance using a judge LLM.")
    parser.add_argument("--model-a", type=str, default="biollama3", help="Name of the baseline model (Model A).")
    parser.add_argument("--model-b", type=str, required=True, help="Name of the new candidate model to evaluate (Model B).")
    parser.add_argument("--judge-model", type=str, default="llama3:70b", help="Name of the judge model.")
    parser.add_argument("--dataset", type=str, default="evaluation_dataset.json", help="Path to the evaluation dataset JSON file.")
    parser.add_argument("--output-log", type=str, default="evaluation_log.json", help="Path to save the detailed evaluation log JSON file.")
    
    args = parser.parse_args()

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_db()

    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(run_evaluation(
        model_a=args.model_a,
        model_b=args.model_b,
        judge_model=args.judge_model,
        eval_dataset_path=args.dataset,
        output_log_path=args.output_log
    ))